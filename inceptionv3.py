from __future__ import print_function
import os
import re
import glob
import random
import numpy as np
import pandas as pd
import csv as csv
import keras
from keras.applications.inception_v3 import InceptionV3
from keras.preprocessing import image
from keras.models import Model
from keras.layers import Dense, GlobalAveragePooling2D
from keras import backend as K
from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img


def sorted_nicely( l ):
    convert = lambda text: int(text) if text.isdigit() else text
    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]
    return sorted(l, key = alphanum_key)

def get_im(path):
    img = load_img(path)
    return img

def load_train():
    X_train = []
    print('Reading training images')
    files = sorted_nicely(glob.glob("train_img/*.png"))
    for f in files:
        img = get_im(f)
        img = np.asarray(img)
        X_train.append(img)
    return X_train

def load_test():
    print('Reading test images')
    files = sorted_nicely(glob.glob("test_img/*.png"))
    X_test = []
    for f in files:
        img = get_im(f)
        img = np.asarray(img)
        X_test.append(img)
    return X_test

X_train = load_train()
X_train = np.asarray(X_train)
print (X_train.shape)
x_train = X_train[:3015]
x_val = X_train[3015:]

x_test = load_test()
x_test = np.asarray(x_test)
print (x_test.shape)

df = pd.read_csv('train.csv', header=0)
labels = df['label']
labels = labels.tolist()
#print (labels)
setLabel = set(labels)
setLabel = list(setLabel)
#print (setLabel)

Y_train = np.zeros((3215, 25))
for i in range(3215):
    for j in range(len(setLabel)):
        if(setLabel[j] == labels[i]):
            Y_train[i][j] = 1
            break

y_train = Y_train[:3015]
y_val = Y_train[3015:]


batch_size = 64
num_classes = 25
epochs = 50
data_augmentation = True
num_predictions = 25


def generator(features, labels, batch_size):

 # Create empty arrays to contain batch of features and labels#

    batch_features = np.zeros((batch_size, 128, 128, 3))#128
    batch_labels = np.zeros((batch_size,1))

    while True:
        for i in range(batch_size):
        # choose random index in features
            index= random.choice(len(features),1)
            batch_features[i] = some_processing(features[index])
            batch_labels[i] = labels[index]
        yield batch_features, batch_labels


# create the base pre-trained model
base_model = InceptionV3(weights='imagenet', include_top=False)

# add a global spatial average pooling layer
x = base_model.output
x = GlobalAveragePooling2D()(x)
# let's add a fully-connected layer
x = Dense(1024, activation='relu')(x)#two FC
# and a logistic layer -- let's say we have 200 classes
predictions = Dense(25, activation='softmax')(x)

# this is the model we will train
model = Model(inputs=base_model.input, outputs=predictions)

# first: train only the top layers (which were randomly initialized)
# i.e. freeze all convolutional InceptionV3 layers
for layer in base_model.layers:
    layer.trainable = False

# compile the model (should be done *after* setting layers to non-trainable)
model.compile(optimizer='adam', loss='categorical_crossentropy')

# train the model on the new data for a few epochs

datagen = ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        rotation_range=30,  # randomly rotate images in the range (degrees, 0 to 180)
        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)#0.1
        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
        horizontal_flip=True,  # randomly flip images
        vertical_flip=True)  # randomly flip images

    # Compute quantities required for feature-wise normalization
    # (std, mean, and principal components if ZCA whitening is applied).
datagen.fit(x_train)

    # Fit the model on the batches generated by datagen.flow().
model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size), steps_per_epoch=x_train.shape[0], epochs=epochs, validation_data=(x_val, y_val))
    

# at this point, the top layers are well trained and we can start fine-tuning
# convolutional layers from inception V3. We will freeze the bottom N layers
# and train the remaining top layers.

# let's visualize layer names and layer indices to see how many layers
# we should freeze:
for i, layer in enumerate(base_model.layers):
   print(i, layer.name)

# we chose to train the top 2 inception blocks, i.e. we will freeze
# the first 249 layers and unfreeze the rest:
for layer in model.layers[:249]:
   layer.trainable = False
for layer in model.layers[249:]:
   layer.trainable = True

# we need to recompile the model for these modifications to take effect
# we use SGD with a low learning rate
from keras.optimizers import SGD
model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')

# we train our model again (this time fine-tuning the top 2 inception blocks
# alongside the top Dense layers

datagen.fit(x_train)
model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size), steps_per_epoch=x_train.shape[0], epochs=epochs, validation_data=(x_val, y_val))


res_arr = model.predict(x_test)# will return a numpy array of shape 1732 x 25
res_arr = np.argmax(res_arr, axis=1) # 1732 x 1
final_label = []
for i in range(1732):
    final_label.append(setLabel[res_arr[i]])
test_df = pd.read_csv('test.csv', header=0)
test_df['label'] = pd.Series(final_label)
test_df.to_csv('submission.csv')